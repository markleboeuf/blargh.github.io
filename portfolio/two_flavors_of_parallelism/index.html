<!DOCTYPE html>
<html lang="en-us">
<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>Two Flavors of Parallel Simulation</title>
<meta name="description" content="Code and Stuff">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="robots" content="all,follow">
<meta name="googlebot" content="index,follow,snippet,archive">
<link rel="stylesheet" href="https://markleboeuf.github.io/css/bootstrap.min.css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:400,300,700,400italic">
<link rel="stylesheet" href="https://markleboeuf.github.io/css/font-awesome.min.css">
<link rel="stylesheet" href="https://markleboeuf.github.io/css/owl.carousel.css">
<link rel="stylesheet" href="https://markleboeuf.github.io/css/owl.theme.css">


  <link href="https://markleboeuf.github.io/css/style.red.css" rel="stylesheet" id="theme-stylesheet">


<link href="https://markleboeuf.github.io/css/custom.css" rel="stylesheet">
<link rel="shortcut icon" href="https://markleboeuf.github.io/img/favicon.png">


</head>
<body>
  <div id="all">
      <div class="container-fluid">
          <div class="row row-offcanvas row-offcanvas-left">
              <div id="sidebar" class="col-xs-6 col-sm-4 col-md-3 sidebar-offcanvas">
  <div class="sidebar-content">
    <h1 class="sidebar-heading"><a href="/">Mark LeBoeuf</a></h1>
    
      <p class="sidebar-p">I am a Data Scientist, currently based in Portland, OR.</p>
    
    <ul class="sidebar-menu">
      
      
        <li><a href="https://markleboeuf.github.io/">Home</a></li>
      
        <li><a href="https://markleboeuf.github.io/about/">About</a></li>
      
    </ul>
    <p class="social">
  
  
  
  <a href="full%20profile%20url%20in%20twitter" data-animate-hover="pulse" class="external twitter">
    <i class="fa fa-twitter"></i>
  </a>
  
  
  
  
  <a href="full%20profile%20url%20in%20linkedin" data-animate-hover="pulse">
    <i class="fa fa-linkedin"></i>
  </a>
  
  
  
  <a href="full%20profile%20url%20in%20github" data-animate-hover="pulse">
    <i class="fa fa-github"></i>
  </a>
  
</p>


    <div class="copyright">
      <p class="credit">
        
          &copy;2016 Mark LeBoeuf
        
        | Template by <a href="https://bootstrapious.com/free-templates" class="external">Bootstrapious.com</a>

&amp; ported to Hugo by <a href="https://github.com/kishaningithub">Kishan B</a>

      </p>
    </div>
  </div>
</div>

              
<div class="col-xs-12 col-sm-8 col-md-9 content-column white-background">
  <div class="small-navbar visible-xs">
  <button type="button" data-toggle="offcanvas" class="btn btn-ghost pull-left"> <i class="fa fa-align-left"> </i>Menu</button>
  <h1 class="small-navbar-heading"><a href="/">Mark LeBoeuf</a></h1>
</div>

  <div class="row">
    <div class="col-lg-8">
      <div class="content-column-content">
         <h1>Two Flavors of Parallel Simulation</h1>
         <p>Tired of waiting around for your simulations to finish? Run them in parallel! This post covers two seperate ways to add  parallelism &ndash; and speed &ndash; to your R code.
</p>

<p><img src="../two_flavors_of_parallelism_images/two_flavors.jpg" class="img-responsive" style="display: block; margin: auto;" /></p>

<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#parallel-simulations-with-foreach">Parallel Simulations with Foreach</a></li>
<li><a href="#parallel-simulations-with-multidplyr">Parallel Simulations with Multidplyr</a></li>
</ul>

<h2 id="overview">Overview</h2>

<p>In a prior post we discussed how to use monte carlo simulation for power analysis. We kept the total number of iterations relatively low (n = 25)  to illustrate the process. However, the real value of simulation emerges when you can run lots of simulations, because the more iterations you run the better idea you get about the thing you are trying to estimate (see <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers"><em>Law of Large Numbers</em></a>). In the case of estimating the power of an experiment, the more simulated experiments we run the closer we&rsquo;ll get to the true probability of committing a <a href="http://support.minitab.com/en-us/minitab/17/topic-library/basic-statistics-and-graphs/hypothesis-tests/basics/type-i-and-type-ii-error/"><em>Type II Error</em></a>. Simulating the experimental paradigm sequentially is fine but it takes a long time when you increase the number of simulations to, say, 10K or 100K. Any time you come across a task that involves repeated sampling from a distribution &ndash; <strong>think parallel</strong>. The results of one simulation do not feed into or depend on the results of another. Thus we can run many simulated experiments at the same time. This is a common theme of any task that is parallelizable, which might be one of the most challenging words to say. In this post I&rsquo;m going to discuss two seperate ways in R to implement a power analysis simulation. And although we&rsquo;ll focus only on paralellism in the context of experimental power, the workflow discussed here can be generalized to almost any task that involves repeated sampling.</p>

<h3 id="parallel-simulations-with-foreach">Parallel Simulations with Foreach</h3>

<p>I&rsquo;ll briefly review the task at hand. Researchers conducted a study examining the impact of continued sleep deprivation (defined as receiving only 3 hours of sleep per night) on reaction time. The study was run for 9 days and the researchers found a significant effect for Number of Days. As you can imagine, participants were a lot slower to react on days 8 &amp; 9 relative to days 0 &amp; 1. We want to replicate this effect but don&rsquo;t have the time to wait 9 days for a result. Our question, then, is whether we could still detect an effect of sleep deprivation after only 3 days. The goal is to achieve at least 80% power, which means that if we replicated the experiment 10 times under the exact same conditions, we would find a significant effect (<em>p</em> &lt; 0.05) in at least 8 of the experiments.</p>

<p>We&rsquo;ll use the findings from the prior study over the first 3 days as our base data set. The process will be modeled with a mixed effects model with a random intercept for each participant. Our fixed effect &ndash; the thing we are interested in &ndash; is days of sleep deprivation. Let&rsquo;s load up our libraries and fit the initial model.</p>

<pre><code class="language-r">libs = c('foreach', 'doParallel', 'lme4', 'dplyr', 'broom', 'ggplot2', 'multidplyr', 'knitr')
lapply(libs, require, character.only = TRUE)
sleep_df = lme4::sleepstudy %&gt;% 
           dplyr::filter(Days %in% c(0, 1, 2))

fit = lmer(Reaction ~ Days + (1|Subject), data = sleep_df)
confidence_intervals = confint(fit)
</code></pre>

<pre><code class="language-r">print(summary(fit))
print(confidence_intevals)
</code></pre>

<pre><code class="language-r">## Linear mixed model fit by REML ['lmerMod']
## Formula: Reaction ~ Days + (1 | Subject)
## Data: sleep_df
##
## REML criterion at convergence: 490.5
##
## Scaled residuals: 
## Min      1Q  Median      3Q     Max 
## -2.9117 -0.4918 -0.0551  0.5073  2.3561 
##
## Random effects:
## Groups   Name        Variance Std.Dev.
## Subject  (Intercept) 667.0    25.83   
## Residual             335.5    18.32   
## Number of obs: 54, groups:  Subject, 18
##
## Fixed effects:
## Estimate Std. Error t value
## (Intercept)  257.815      7.252   35.55
## Days           4.355      3.053    1.43
##
## Correlation of Fixed Effects:
## (Intr)
## Days -0.421

## Computing profile confidence intervals ...
##                  2.5 %    97.5 %
## .sig01       17.107331  37.72231
## .sigma       14.574335  23.19585
## (Intercept) 243.425257 272.20429
## Days         -1.705688  10.41578
</code></pre>

<p>Our model indicates that after controlling for baseline differences in participant reaction time (i.e., our random intercept), each additional day increases reaction time by about 4 seconds (4.355 to be exact). Our confidence interval for this coefficient indicates a non-significant effect, as the range contains zero. However, the range is fairly wide. Let&rsquo;s determine how this uncertainty in our parameter estimate affects overall experimental power. We&rsquo;ll make predictions on our base dataset with the model defined above, and then add noise (defined by our residuals from our initial model fit) to simulate the sampling process.</p>

<pre><code class="language-r">model_predictions = predict(fit, sleep_df)

standard_deviation = sd(fit@resp$y - fit@resp$mu)

n_simulations = 100
</code></pre>

<h3 id="testing-power-with-foreach">Testing Power With foreach</h3>

<p><code>For</code> loops in R are great for small operations but are the absolute worst for larger operations. Enter <code>foreach</code>. The syntax is a little bit different from your typical <code>for</code> loop. Let&rsquo;s first see how to implement our power simulation <strong>sequentially</strong> using <code>foreach</code>. Note that this approach is identical to using a regular <code>for</code> loop.</p>

<pre><code class="language-r">sequential_start_time &lt;- Sys.time()

sequential_results = foreach(
                        i = 1:n_simulations,
                        .combine = &quot;rbind&quot;,
                        .packages = c(&quot;lme4&quot;, &quot;broom&quot;, &quot;dplyr&quot;)) %do% {
                        # generate residuals
                        temporary_residuals = rnorm(nrow(sleep_df), mean = 0, sd = standard_deviation)

                        #create simulated reaction time
                        sleep_df$Simulated_Reaction &lt;- model_predictions + temporary_residuals

                        #refit our model on the simulated data
                        temp_fit = lmer(Simulated_Reaction ~ Days + (1|Subject), data = sleep_df)

                        # return confidence interval for the Days coefficient
                        tidy(confint(temp_fit)) %&gt;%
                        dplyr::rename(coefficients = .rownames,
                        lower_bound = X2.5..,
                        upper_bound = X97.5..) %&gt;%
                        dplyr::filter(coefficients == 'Days') %&gt;%
                        dplyr::select(lower_bound, upper_bound)
}

sequential_end_time &lt;- Sys.time()
sequential_run_time &lt;- sequential_end_time - sequential_start_time

print(paste0(&quot;TOTAL RUN TIME: &quot;, round(sequential_run_time), &quot; SECONDS&quot;))
</code></pre>

<pre><code>## [1] &quot;TOTAL RUN TIME: 49 SECONDS&quot;
</code></pre>

<p>Implementing the power simulation sequentially took 49 seconds on my computer. Let&rsquo;s compare that to a parallel implementation. All we have to do is change the <code>%do%</code> to  <code>%dopar%</code> to shift the execution from sequential to parallel. But first we&rsquo;ll have to set up a computing cluster.</p>

<pre><code class="language-r"># register our cluster
cl &lt;- makeCluster(detectCores() - 1)

registerDoParallel(cl)
</code></pre>

<p>My computing cluster has 7 cores. I have a total of 8 cores on my machine, but I want to save one for browsing the internet, cat gifs, etc. Now that we&rsquo;ve registered our cluster, let&rsquo;s re-run the above code block but replace <code>%do%</code> with <code>%dopar%</code> and compare the run time.</p>

<pre><code class="language-r">parallel_start_time &lt;- Sys.time()
parallel_results = foreach(
                           i = 1:n_simulations,
                           .combine = &quot;rbind&quot;,
                           .packages = c(&quot;lme4&quot;, &quot;broom&quot;, &quot;dplyr&quot;)) %dopar% {
                           # generate residuals
                           temporary_residuals = rnorm(nrow(sleep_df), mean = 0, sd = standard_deviation)

                           #create simulated reaction time
                           sleep_df$Simulated_Reaction = model_predictions + temporary_residuals

                           #refit our model on the simulated data
                           temp_fit = lmer(Simulated_Reaction ~ Days + (1|Subject), data = sleep_df)

                           #return confidence interval for the Days coefficient
                           tidy(confint(temp_fit)) %&gt;%
                           dplyr::rename(coefficients = .rownames,
                           lower_bound = X2.5..,
                           upper_bound = X97.5..) %&gt;%
                           dplyr::filter(coefficients == 'Days') %&gt;%
                           dplyr::select(lower_bound, upper_bound)
}

parallel_end_time &lt;- Sys.time()
parallel_run_time &lt;- parallel_end_time - parallel_start_time

print(paste0(&quot;TOTAL RUN TIME: &quot;, round(parallel_run_time), &quot; SECONDS&quot;))
</code></pre>

<pre><code class="language-r">## [1] &quot;TOTAL RUN TIME: 12 SECONDS&quot;
</code></pre>

<p>So that only took 12 seconds, which is a 68% reduction in runtime! That means 68% more time to to view cat gifs or do other, super-important activities. Let&rsquo;s check and see how our power calculations panned out. Every instance in which we find a zero in our confidence interval for the Days estimate is a type II error.</p>

<pre><code class="language-r">power_results = parallel_results %&gt;% 
                dplyr::mutate(row_index = 1:nrow(parallel_results)) %&gt;% 
                dplyr::group_by(row_index) %&gt;% 
                dplyr::do(result = dplyr::between(0, .$lower_bound, .$upper_bound) %&gt;% 
                dplyr::mutate(result = as.integer(unlist(result))) %&gt;% 
                data.frame()

print(paste0(&quot;TOTAL POWER: &quot;, (n_simulations - sum(power_results$result)), &quot;%&quot;))

</code></pre>

<pre><code class="language-r">## [1] &quot;TOTAL POWER: 47%&quot;
</code></pre>

<p>If we ran our experiment under these conditions, we&rsquo;d detect an effect that we know exists in about 1 of every 2 experiments. This isn&rsquo;t ideal. The next step here would be to further increase the number of days or participants, but I don&rsquo;t want to focus too much on the experimental design. Instead, let&rsquo;s move on to the second approach to parallelism that keeps all of our operations in the <code>tidyverse</code>, which is total R programming zen.</p>

<h3 id="parallel-simulation-with-multidplyr">Parallel Simulation with multidplyr</h3>

<p>If you haven&rsquo;t used dplyr before, I would strongly suggest learning it. It is a huge boon for productivity, as you can express all of your data munging operations in clean, easy to read syntax. <code>Multidplyr</code> builds on <code>dplyr</code> by allowing operations to performed in parallel. It is a natural fit when you have grouped data and want to apply the same function to each group. The groups in our data will be each sampling iteration. I&rsquo;ll go through line by line and explain what&rsquo;s happening.</p>

<p>Here we are going to make 100 copies of our dataset and bind them together. We&rsquo;ll also generate all the errors for each of the iterations and bind that to our 100 copies of the dataset.</p>

<pre><code class="language-r">sleep_df_copy = data.frame(sapply(sleep_df, rep.int, times = n_simulations))

temporary_residuals = c()
for(i in 1:n_simulations){
    temporary_residuals = c(temporary_residuals, rnorm(nrow(sleep_df), mean = 0, sd = standard_deviation))
}
sleep_df_copy$iteration = rep(1:n_simulations, each = nrow(sleep_df))

sleep_df_copy$Simulated_Reaction = temporary_residuals + rep(model_predictions, n_simulations)
</code></pre>

<p>At this point each study has 54 observations (18 participants with 3 data points each). We created 100 replications of the study, so our total dataset size is now 5400 rows. Each 54 observation &ldquo;group&rdquo; is identified by the <code>iteration</code> field. This means that each core should receive approximately 14 iterations (<sup>100</sup>&frasl;<sub>7</sub>) with 54 observations per iteration, for a total of around 756 observations. The data structure that holds the data partitions is called a <code>party_df</code> for <code>partitioned data frame</code>..or maybe because all of the cores can join the party..I&rsquo;m not sure, honestly. Let&rsquo;s create one and examine the distribution of our observations.</p>

<pre><code class="language-r">partitioned_experiment &lt;- multidplyr::partition(sleep_df_copy, iteration)

##Source: party_df [5,400 x 6]
##Groups: iteration
##Shards: 7 [756--810 rows]

### S3: party_df
##   Reaction  Days Subject iteration  residuals Simulated_Reaction
##      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;      &lt;dbl&gt;              &lt;dbl&gt;
##1  249.5600     0       1         2  1.9977383           251.5577
##2  258.7047     1       1         2  6.4680182           265.1727
##3  250.8006     2       1         2  3.7092064           254.5098
##4  222.7339     0       2         2  0.5424556           223.2764
##5  205.2658     1       2         2 10.3575587           215.6234
##6  202.9778     2       2         2  9.4995648           212.4774
##7  199.0539     0       3         2  9.0741097           208.1280
##8  194.3322     1       3         2 -0.4365952           193.8956
##9  234.3200     2       3         2  7.7861938           242.1062
##10 321.5426     0       4         2 -5.5537177           315.9889
### ... with 5,390 more rows
</code></pre>

<p>Sweet! <code>Multidplyr</code> has producted a relatively even split of our dataset, which is exactly what we want. Next we&rsquo;ll load up the libraries and pass the function we want to apply to each of our partitions. In this case we&rsquo;ve defined it as <code>calculateCI</code>.</p>

<pre><code class="language-r"># Create function to calculate confidence intervals
calculateCI = function(sleep_data){
    # fit the model
    temp_fit &lt;- lmer(Simulated_Reaction ~ Days + (1|Subject), data = sleep_data)

    # return the coefficients
    tidy(confint(temp_fit)) %&gt;% 
    dplyr::rename(coefficients = .rownames,
    lower_bound = X2.5..,
    upper_bound = X97.5..) %&gt;% 
    dplyr::filter(coefficients == 'Days') %&gt;% 
    dplyr::select(lower_bound, upper_bound)

}

# load our libraries and calculateCI function on each of the cores
partitioned_experiment %&gt;% 
    cluster_library(&quot;lme4&quot;) %&gt;% 
    cluster_library(&quot;dplyr&quot;) %&gt;% 
    cluster_library(&quot;broom&quot;) %&gt;% 
    cluster_assign_value(&quot;calculateCI&quot;, calculateCI)

</code></pre>

<p>That&rsquo;s it. We are now ready to run our simulated power experiment. We&rsquo;ll use the <code>collect</code> verb to bring the partitioned results back. We&rsquo;ll also do a little list processing, as the results are returned in a list format.</p>

<pre><code class="language-r">multidplyr_start_time &lt;- Sys.time()

multidplyr_results &lt;- partitioned_experiment %&gt;% 
dplyr::do(results = calculateCI(.)) %&gt;% 
          collect() %&gt;% 
          dplyr::mutate(lower_bound = unlist(lapply(results, function(x) x[[1]][1])),
          upper_bound = unlist(lapply(results, function(x) x[[2]][1]))) %&gt;%
          dplyr::select(-results) %&gt;% 
          data.frame()

multidplyr_end_time &lt;- Sys.time()
multidplyr_run_time &lt;- multidplyr_end_time - multidplyr_start_time

print(paste0(&quot;TOTAL RUN TIME: &quot;, round(multidplyr_run_time)))
</code></pre>

<pre><code class="language-r">## [1] &quot;TOTAL RUN TIME: 13 SECONDS&quot;
</code></pre>

<p>Run time is almost identical to <code>foreach</code>. Let&rsquo;s check out the power estimate.</p>

<pre><code class="language-r">power_results = multidplyr_results %&gt;% 
                dplyr::mutate(row_index = 1:nrow(parallel_results)) %&gt;% 
                dplyr::group_by(row_index) %&gt;% 
                dplyr::do(result = dplyr::between(0, .$lower_bound, .$upper_bound)) %&gt;% 
                dplyr::mutate(result = as.integer(unlist(result))) %&gt;% 
                data.frame()

print(paste0(&quot;TOTAL POWER: &quot;, (n_simulations - sum(power_results$result)), &quot;%&quot;))
</code></pre>

<pre><code>## [1] &quot;TOTAL POWER: 39%&quot;
</code></pre>

<p>This simulation gave us a lower power estimate of 39% relative to 47%, despite the fact that all of the underlying calculations were identical to those implemented with <code>foreach</code>. This is main reason why implementing a large number of simulations is paramount, because we&rsquo;ll get a much better idea of the thing we&rsquo;re trying to estimate as the number of iterations increases. If you want to see for yourself, increase the number of iterations from 100 to 10000, and then run the process again with both <code>Multidplyr</code> &amp; <code>Foreach</code>. The power estimates from each will be identical. I tried it and came out with an estimate of 46% for both (after some serious waiting time). When you are finished, remember to shut your cluster down with the following command.</p>

<pre><code class="language-r">stopCluster(cl)
</code></pre>

<p>I hope this post provides you with better idea of how easy it is to add some parallelism to your R code. Time always seems to be in short supply when you are developing, and waiting around for an answer is a total momentum killer. Taking a bit more time up front to understand whether you can run your code in parallel &ndash; and avoiding sequential for loops &ndash; will save you a ton of time down the line.</p>
         
      </div>
    </div>
  </div>
</div>

          </div>
      </div>
  </div>
  <script src="https://markleboeuf.github.io/js/jquery.min.js"></script>
<script src="https://markleboeuf.github.io/js/bootstrap.min.js"></script>
<script src="https://markleboeuf.github.io/js/jquery.cookie.js"> </script>
<script src="https://markleboeuf.github.io/js/ekko-lightbox.js"></script>
<script src="https://markleboeuf.github.io/js/jquery.scrollTo.min.js"></script>
<script src="https://markleboeuf.github.io/js/masonry.pkgd.min.js"></script>
<script src="https://markleboeuf.github.io/js/imagesloaded.pkgd.min.js"></script>
<script src="https://markleboeuf.github.io/js/owl.carousel.min.js"></script>
<script src="https://markleboeuf.github.io/js/front.js"></script>

</body>
</html>
