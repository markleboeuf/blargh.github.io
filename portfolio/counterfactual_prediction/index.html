<!DOCTYPE html>
<html lang="en-us">
<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>Establishing Causality with Counterfactual Prediction</title>
<meta name="description" content="...">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="robots" content="all,follow">
<meta name="googlebot" content="index,follow,snippet,archive">
<link rel="stylesheet" href="/css/bootstrap.min.css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:400,300,700,400italic">
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/owl.carousel.css">
<link rel="stylesheet" href="/css/owl.theme.css">


  <link href="/css/style.red.css" rel="stylesheet" id="theme-stylesheet">


<link href="/css/custom.css" rel="stylesheet">
<link rel="shortcut icon" href="/img/favicon.png">


<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-101855339-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='//www.google-analytics.com/analytics.js'></script>

</head>
<body>
  <div id="all">
      <div class="container-fluid">
          <div class="row row-offcanvas row-offcanvas-left">
              <div id="sidebar" class="col-xs-6 col-sm-4 col-md-3 sidebar-offcanvas">
  <div class="sidebar-content">
    <h1 class="sidebar-heading"><a href="/">Mark LeBoeuf</a></h1>
    
      <p class="sidebar-p">I am a Data Scientist currently based in Portland, OR.</p>
    
    <ul class="sidebar-menu">
      
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/about/">About</a></li>
      
    </ul>
    <p class="social">
  
  
  
  
  
  
  <a href="https://www.linkedin.com/in/markleboeuf/" data-animate-hover="pulse">
    <i class="fa fa-linkedin"></i>
  </a>
  
  
  
  <a href="https://github.com/markleboeuf/markleboeuf.github.io" data-animate-hover="pulse">
    <i class="fa fa-github"></i>
  </a>
  
</p>


    <div class="copyright">
      <p class="credit">
        
          &copy;2017 Mark LeBoeuf
        
        | Template by <a href="https://bootstrapious.com/free-templates" class="external">Bootstrapious.com</a>

&amp; ported to Hugo by <a href="https://github.com/kishaningithub">Kishan B</a>

      </p>
    </div>
  </div>
</div>

              
<div class="col-xs-12 col-sm-8 col-md-9 content-column white-background">
  <div class="small-navbar visible-xs">
  <button type="button" data-toggle="offcanvas" class="btn btn-ghost pull-left"> <i class="fa fa-align-left"> </i>Menu</button>
  <h1 class="small-navbar-heading"><a href="/">Mark LeBoeuf</a></h1>
</div>

  <div class="row">
    <div class="col-lg-8">
      <div class="content-column-content">
         <h1>Establishing Causality with Counterfactual Prediction</h1>
         <p>Sometimes a controlled experiment isn&rsquo;t an option yet you want to establish causality. This post outlines a method for quantifying the effects of an intervention via counterfactual predictions.</p>

<p></p>

<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#souvenir-sales-australia-and-wacky-inflatable tube-men">Souvenir Sales, Australia, and Wacky Inflatable Tube Men</a></li>
<li><a href="#creating-the-synthetic-data-set">Creating the Synthetic Data Set</a></li>
<li><a href="#selecting-a-control-time-series-with-dynamic-time-warping">Selecting a Control Time Series with Dynamic Time Warping</a></li>
<li><a href="#generating-counterfactual-predictions">Generating Counterfactual Predictions</a></li>
</ul>

<h2 id="overview">Overview</h2>

<p>Businesses have several &ldquo;levers&rdquo; they can pull in an effort to boost sales. For example, a series of advertisements might be shown in a particular city or region with the hopes of increasing sales for the advertised product. Ideally the cost associated with the advertisement would be outstripped by the expected increase in sales, visits, conversion, or whatever KPI (key performance indicator) the business hopes to drive. But how do you capture the ROI of the advertisement? In a parallel universe, where all experiments are possible, we would create two copies of the same city, run the advertisement in one of them, track our metric of interest over the some period of time, and then compare the difference  between the two cities. All potential confounding variables, such as differences in seasonal variation, viewership rates, or buying preference, would be controlled. Thus any difference (lift) in our KPI could be attributed to the intervention.</p>

<h3 id="souvenir-sales-australia-and-wacky-inflatable-tube-men">Souvenir Sales, Australia, and Wacky Inflatable Tube Men</h3>

<p>We can&rsquo;t clone cities, but there is a way to statistically emulate the above situation. This post provides a general overview of how to generate a <strong>counterfactual prediction</strong>, which is a way to quantify what would&rsquo;ve happened had we never run the advertisement, event, or intervention.</p>

<p>Incremental Sales, ROI, KPIs, Lift &ndash; these terms are somewhat abstract. Australia, Souvenirs, Wacky- Inflatable-Tube-Men (WITM for short) &ndash; that all seems pretty straight-forward. So imagine you run a souvenir shop near the beach in Australia and you&rsquo;re looking for ways to attract additional foot traffic and (hopefully) increase sales. You decide to purchase this guy below, hoping it will draw attention to your store, increase foot traffic, and produce more souvenir sales.</p>

<p><img src="../counterfactual_prediction_images/wacky_inflatable_man.jpg" class="img-responsive" style="display: block; margin: auto;" /></p>

<p>Being a savy business owner, you&rsquo;re interested in the effect of your new WITM on sales. Does having this form of advertisement actually drive sales beyond what would&rsquo;ve happened in the absence of any WITM? You look to the other six souvenir shops in seperate parts of town to answer this question. The other souvenir shops will serve as a control group because they don&rsquo;t have a WITM. Additionally, the other shops are located far enough away from your shop so there is no way for their sales to be affected by your WITM (this is a critical assumption that in real-world contexts should be verified). After accounting for base-line differences in sales between your shop and the control shops, you build a forecast &ndash; or counterfactual prediction &ndash; premised on what would&rsquo;ve happened in your shop had the WITM intervention never taken place. This prediction will then serve as the baseline from which to compare what actually happened to sales.</p>

<p>In short, you (the shop owner) are faced with two tasks:</p>

<ol>
<li>Identifying which of control shop(s) are most similar to your shop</li>
<li>Generating the counterfactual prediction</li>
</ol>

<p>I&rsquo;ll go through each of these steps at a high level, just to give you a general idea of logic underlying the process. For an excellent, more in-depth explanation of the technical details check out this <a href="http://multithreaded.stitchfix.com/blog/2016/01/13/market-watch/">post</a> by the author of the <code>MarketMatching</code> package, which we will leverage to address both tasks.</p>

<h3 id="creating-the-synthetic-data-set">Creating the Synthetic Data Set</h3>

<p>Let&rsquo;s first download the dataset and load up the required libraries. You can find the dataset <a href="http://robjhyndman.com/tsdldata/data/fancy.dat">here</a>.</p>

<pre><code class="language-r">libs = c(&quot;devtools&quot;, &quot;CausalImpact&quot;, &quot;forecast&quot;, 
         &quot;dplyr&quot;, &quot;forcats&quot;, &quot;MarketMatching&quot;, 
         &quot;knitr&quot;, &quot;ggplot2&quot;, &quot;artyfarty&quot;)

lapply(libs, require, character.only = TRUE)

# Uncomment if you dont have the package installed 
#devtools::install_github(&quot;klarsen1/MarketMatching&quot;, build_vignettes=TRUE)
#library(MarketMatching)

file_name = 'monthly_souvenir_sales.csv'
working_dir = 'your_working_directory'

souvenir = read.csv(paste0(working_dir, file_name))

names(souvenir) = c(&quot;Month&quot;, &quot;Sales&quot;)

souvenir$Date = as.Date(paste0(as.character(souvenir$Month), &quot;-01&quot;),
                        format = &quot;%Y-%m-%d&quot;)
</code></pre>

<p>The dataset is total monthly souvenir sales for your shop, starting in January of 1987 and running through December of 1993. Let&rsquo;s take a high level glance at our time series to see if we need to clean it up.</p>

<pre><code class="language-r">color_values = c(&quot;#D9D9D9&quot;, &quot;#33C5F3&quot;, &quot;#F16C62&quot;, &quot;#A8CE38&quot;, &quot;#FCB51E&quot;, &quot;#F92672&quot;)

my_plot_theme = function(){
  font_family = &quot;Helvetica&quot;
  font_face = &quot;bold&quot;
  return(theme(
    axis.text.x = element_text(size = 18, face = font_face, family = font_family),
    axis.text.y = element_text(size = 18, face = font_face, family = font_family),
    axis.title.x = element_text(size = 20, face = font_face, family = font_family),
    axis.title.y = element_text(size = 20, face = font_face, family = font_family),
    strip.text.y = element_text(size = 18, face = font_face, family = font_family),
    plot.title = element_text(size = 24, face = font_face, family = font_family),
    legend.position = &quot;top&quot;,
    legend.title = element_text(colour = &quot;white&quot;, size = 16,
                                face = font_face,
                                family = font_family),
    legend.text = element_text(colour = &quot;white&quot;, size = 14,
                               face = font_face,
                               family = font_family)
  ))
}

ggplot(souvenir, aes(x = Date, y = Sales)) + geom_point(color = color_values[1], 
                                                        size = 2) + 
                                             geom_line(color = color_values[1],
                                                       size = 2) + 
  theme_monokai_full() + 
  my_plot_theme()
</code></pre>

<p><img src="../counterfactual_prediction_images/sales_plot_1.png" class="img-responsive" style="display: block; margin: auto;" /></p>

<p>Notice that the seasonal changes become more volatile across time, particularly in the past couple years. The forecasting model we&rsquo;ll be using is additive and thus cannot account for a multiplicative time series, so we&rsquo;ll need to log-transform sales before we build a model.</p>

<pre><code>souvenir_ts = ts(log(souvenir$Sales),
                 frequency = 12,
                 start = c(1987, 1))
</code></pre>

<p>Now our seasonal fluctuations are roughly constant across time and can be described with an additive model. Next we&rsquo;ll introduce some random noise to the time series and then generate a 6-month ahead forecast. A constant (0.5 SDs) will be added to each of the forecasted values. This is to simulate the positive effect that the WITM intervention had on sales during the 6-month intervention period. We&rsquo;ll then plot the results to get a better idea of when each of aforementioned events is happening.</p>

<pre><code class="language-r">set.seed(123)
# add noise to log-transformed sales
souvenir_ts = souvenir_ts + rnorm(length(souvenir_ts), 0, (sd(souvenir_ts) * 0.25))

# define length of intervention
trial_period_months = 6

simulated_intervention = forecast(auto.arima(souvenir_ts), trial_period_months)
# define effect size
effect_size = 0.5

# create simulated effect
simulated_intervention = simulated_intervention$mean + (sd(souvenir_ts) * effect_size)

monthly_log_sales = c(souvenir_ts, simulated_intervention)

study_dates = seq(max(souvenir$Date), 
                  by = &quot;month&quot;, 
                  length.out = (trial_period_months + 1))

study_dates = study_dates[2:length(study_dates)]
intervention_start = study_dates[1]

souvenir_sim = data.frame(measurement_date = c(souvenir$Date, study_dates),
                          sales = c(souvenir_ts, simulated_intervention),
                          store = &quot;1&quot;)
</code></pre>

<pre><code class="language-r">
ggplot(souvenir_sim, aes(x = measurement_date, y = sales)) + 
  theme_monokai_full() +
  my_plot_theme() + ylab(&quot;Log Sales&quot;) + xlab(&quot;Date&quot;) + 
  annotate(&quot;rect&quot;, xmin = min(souvenir_sim$measurement_date), 
           xmax = intervention_start,
           ymin = min(souvenir_sim$sales),
           ymax = Inf,
           fill = color_values[2],
           alpha = 0.2
  ) + 
  annotate(&quot;rect&quot;, xmin = intervention_start, 
           xmax = max(souvenir_sim$measurement_date),
           ymin = min(souvenir_sim$sales),
           ymax = Inf,
           fill = color_values[3],
           alpha = 0.2
  ) + 
  geom_point(color = color_values[1],
             size = 2) +
  geom_line(color = color_values[1],
            size = 2)
</code></pre>

<p><img src="../counterfactual_prediction_images/log_sales_2.png" class="img-responsive" style="display: block; margin: auto;" /></p>

<p>The blue shaded region is the time before the intervention, while the red shaded region is the time during the intervention. It&rsquo;s hard to determine if the level (mean) has increased for the last 6 months of our time series. The uptick could be due to seasonal variation or where the trend of the series was naturally going (Obviously we know this to be false :) ). We want to rule these explanations out. In order to do so, we&rsquo;ll make a counterfactual prediction based on the other control shops.</p>

<p>In this example we&rsquo;ll use the original time series as a basis to create six other time-series. Three will be the original time series with an added gaussian error term while the other three will be random walks. All six will serve as initial candidates for our control shops. Let&rsquo;s generate the time-series for our comparison shops.</p>

<pre><code class="language-r"># create dataframe to hold the 6 reference stores
similar_df = data.frame(NULL)

similiar_stores = c(&quot;2&quot;, &quot;3&quot;, &quot;4&quot;)

# generate time series for similar stores
for(i in 1:length(similiar_stores)){
  temp_ts = souvenir_ts

  # add a bit of random noise
  temp_ts = data.frame(sales = temp_ts + rnorm(length(temp_ts), 
                                                0, 
                                              (sd(temp_ts) * 0.25)))

  # fit a arima model to the simulated sales data
  temp_fit = auto.arima(ts(temp_ts$sales, frequency = 12, 
                        start = c(1987, 1)))
  
  # generate forecast
  forecasted_values = data.frame(forecast(temp_fit, h = trial_period_months))$Point.Forecast

  temp_ts = data.frame(sales = c(temp_ts$sales, forecasted_values),
                       store = similiar_stores[i])

  similar_df = rbind(similar_df, temp_ts)
}

# generate time series for random stores
random_stores = c(&quot;5&quot;, &quot;6&quot;, &quot;7&quot;)

for(r in random_stores){
  temp_random = data.frame(sales = rnorm((length(souvenir_ts) + trial_period_months), 
                                                     mean = mean(souvenir_ts), 
                                                     sd = sd(souvenir_ts)),
           store = r
           )
similar_df = rbind(similar_df, temp_random)
}

# create dataframe to hold all of the time series
similar_df$measurement_date = rep(seq(as.Date(&quot;1987-01-01&quot;), 
                                      by = &quot;month&quot;, 
                                      length.out = nrow(souvenir_sim)), 
                                  length(unique(similar_df$store)))
#
similar_df = rbind(souvenir_sim, similar_df)
</code></pre>

<p>We now have all of time series in a neat dataframe. Let&rsquo;s visualize what that looks like.</p>

<pre><code class="language-r">ggplot(similar_df, aes(x = as.Date(measurement_date), y = sales, color = store)) + geom_point() + geom_line() +
  facet_wrap(~store, ncol = 1) + 
  theme_monokai_full() + 
  my_plot_theme() + ylab(&quot;Sales&quot;) + 
  xlab(&quot;Date&quot;) + 
  theme(strip.text.x = element_text(size = 14, face = &quot;bold&quot;),
        axis.text.y = element_blank(),
        legend.position = &quot;none&quot;)
</code></pre>

<p><img src="../counterfactual_prediction_images/all_7_sales_3.png" class="img-responsive" style="display: block; margin: auto;" /></p>

<p>Store 1 is our store, while stores 2-7 are the potential control stores. In order to make inferences about the effect of our  intervention, we want to identify a seperate store(s) with similar sales history to serve as the control store(s). We&rsquo;ll keep it simple here and only use a single store, but you could potentially use any number of stores as a control. I&rsquo;ll discuss in the next section how we go about defining and identifying similarity.</p>

<h3 id="selecting-a-control-time-series-with-dynamic-time-warping">Selecting a Control Time Series with Dynamic Time Warping</h3>

<p>We&rsquo;ll implement a technique known as <strong>Dynamic Time Warping</strong>. It sounds like something you&rsquo;d hear in a Steven Hawking TED talk, but it&rsquo;s just a way to measure the similarity between two sequences. A common approach for measuring the similarity between sequences is to take the squared or absolute difference between them at the same period in time and then sum up the results. The main drawback with this approach is that it fails to account for seasonal changes that might be shifted or delayed but still occur in the same order. It&rsquo;s like if two people said the same sentence but one person said it much slower than the other. The order and content of the utterance is the same but the cadence is different. This is where Dynamic Time Warping shines. It stretches or compresses (within some constraints) one time series to make it as similar as possible to the other. So an individual&rsquo;s speech cadence wouldn&rsquo;t affect our ability to determine the similarity between two seperate utterances.</p>

<p>In the current context we aren&rsquo;t concerned with leading or lagging seasonality, as a random error was added to each value in the absence of any forward or backward shifts. However, this can be an issue when dealing with phenomena that are impacted by, say, weather. For example, imagine you sell a seasonal product like ice cream. Ice cream sales go up when the weather gets hot, and perhaps it warms up in some parts of the same region before others. This is a case when you might see the exact same sales patterns but some emerge a few months before or after others. Therefore, it is important to use a matching method that can account for these shifts when comparing the similiarity of two time-series.</p>

<p>We&rsquo;ll leverage the <code>MarketMatching</code> package to select our control time series. The selection process is done via DTW. We&rsquo;ll also transform sales back to the original raw values prior to any market-matching/inference.</p>

<pre><code class="language-r">
# exponentiate sales to transform log sales back to original units
similar_df$sales = exp(similar_df$sales)

most_similar_store = MarketMatching::best_matches(data = similar_df,
                             id_variable = &quot;store&quot;,
                             date_variable = &quot;measurement_date&quot;,
                             matching_variable = &quot;sales&quot;,
                             warping_limit = 3,
                             matches = 1,
                             start_match_period = min(similar_df$measurement_date),
                             end_match_period = unique(similar_df$measurement_date)[(length(unique(similar_df$measurement_date)) - 
                                                                                       trial_period_months)]
                             )
</code></pre>

<p><img src="../counterfactual_prediction_images/matches_4.png" class="img-responsive" style="display: block; margin: auto;" /></p>

<p>That was too easy! This table indicates that the pre-intervention sales for Store Number 3 are the most similar to those in Store Number 1. Thus we&rsquo;ll use Store 3 to generate our counterfactual prediction.</p>

<h3 id="generating-counterfactual-predictions">Generating Counterfactual Predictions</h3>

<p>This is the inference part &ndash; namely, can we infer that our intervention impacted sales?</p>

<pre><code class="language-r">results = MarketMatching::inference(matched_markets = most_similar_store,
                                     test_market = &quot;1&quot;,
                                     end_post_period = max(similar_df$measurement_date))
print(results)                                     
</code></pre>

<pre><code class="language-r">##  ------------- Inputs -------------
##  Test Market: 1
##  Control Market 1: 3
##  Market ID: store
##  Date Variable: measurement_date
##  Matching (pre) Period Start Date: 1987-01-01
##  Matching (pre) Period End Date: 1993-12-01
##  Post Period Start Date: 1994-01-01
##  Post Period End Date: 1994-06-01
##  Matching Metric: sales
##  Local Level Prior SD: 0.01
##  Posterior Intervals Tail Area: 95%
## 
## 
##  ------------- Model Stats -------------
##  Matching (pre) Period MAPE: 16.97%
##  Beta 1 [3]: 0.9482
##  DW: 2.14
## 
## 
##  ------------- Effect Analysis -------------
##  Absolute Effect: 60852.26 [35523.48, 88542.17]
##  Relative Effect: 49.5% [28.9%, 72.03%]
##  Probability of a causal impact: 99.8983%
</code></pre>

<p>Let&rsquo;s breakdown the <strong>Effect Analysis</strong> portion of the output. The <strong>Absolute Effect</strong> indicates a lift of approximately 60.8K in sales over the 6-month intervention period. Assuming we paid our $500 for our WITM, the ROI here is huge. The <strong>Relative Effect</strong> provides a standardized view. Most real-world interventions involving people provide a few percentage-points lift; in this case we observed an estimated lift of 49.5%, which is extremely rare. Finally, the <strong>Probability of a causal impact</strong> indicates that the likelihood of observing this effect by chance is extremely low (e.g., 100 - 99.89). We can see what was just described above in visual format below.</p>

<p><img src="../counterfactual_prediction_images/similiar_market_5.png" class="img-responsive" style="display: block; margin: auto;" /></p>

<p>This drives home that our intervention had a clear effect. But wait! The model used to generate the counterfactual prediction above assumed an additive time-series, which is apparent from the fitted model (i.e., it systematically under/overestimated the big changes). We transformed sales back to the original units for ease-of-interpretation, but we would&rsquo;ve achieved a better fit &ndash; and in turn more precise estimates &ndash; had we used the log of sales. Let&rsquo;s see how our estimates change when we use log sales as opposed to raw sales.</p>

<pre><code class="language-r"># log sales 
similar_df$sales = log(similar_df$sales)

most_similar_store = MarketMatching::best_matches(data = similar_df,
                             id_variable = &quot;store&quot;,
                             date_variable = &quot;measurement_date&quot;,
                             matching_variable = &quot;sales&quot;,
                             warping_limit = 3,
                             matches = 1,
                             start_match_period = min(similar_df$measurement_date),
                             end_match_period = unique(similar_df$measurement_date)[(length(unique(similar_df$measurement_date)) - 
                                                                                       trial_period_months)]
                             )

results = MarketMatching::inference(matched_markets = most_similar_store,
                                     test_market = &quot;1&quot;,
                                     end_post_period = max(similar_df$measurement_date))
print(results)                                     
</code></pre>

<pre><code class="language-r">##  ------------- Inputs -------------
##  Test Market: 1
##  Control Market 1: 3
##  Market ID: store
##  Date Variable: measurement_date
##  Matching (pre) Period Start Date: 1987-01-01
##  Matching (pre) Period End Date: 1993-12-01
##  Post Period Start Date: 1994-01-01
##  Post Period End Date: 1994-06-01
##  Matching Metric: sales
##  Local Level Prior SD: 0.01
##  Posterior Intervals Tail Area: 95%
## 
## 
##  ------------- Model Stats -------------
##  Matching (pre) Period MAPE: 1.52%
##  Beta 1 [3]: 0.9231
##  DW: 2.03
## 
## 
##  ------------- Effect Analysis -------------
##  Absolute Effect: 2.68 [1.39, 4.07]
##  Relative Effect: 4.53% [2.36%, 6.87%]
##  Probability of a causal impact: 99.7951%

</code></pre>

<p>Using log-sales instead of raw sales substantially improved the fit of our model. We know this by comparing the MAPE, or Mean Average Percent Error, of our log-transformed model (1.52%) relative to our raw model (16.97%). In this case a lower MAPE indicates a better model fit. Thus, we should consider the estimates from our log-transformed data to be more reliable than those from the previous, raw data. It is interesting to note how our interpretation changes so much! Instead of a 49.5% lift, the estimate is now 4.5%. Finally let&rsquo;s visualize the updated estimate below.</p>

<p><img src="../counterfactual_prediction_images/similiar_market_6.png" class="img-responsive" style="display: block; margin: auto;" /></p>

<p>Hopefully this post gave you a better idea of how to quantify the impact of an intervention. The ability to run a true randomized control study in the real-world is often too expensive or simply not possible. In many cases creating a statistical control group is the best (and cheapest) option available. For more information about the forecasting methodology used produce counterfactual predictions, check out the <a href="https://cran.r-project.org/web/packages/bsts/bsts.pdf">bsts</a> and <a href="https://google.github.io/CausalImpact/CausalImpact.html">CausalImpact</a> packages. Happy experimenting!</p>
         <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'https-markleboeuf-github-io\/';
    var disqus_identifier = '\/portfolio\/counterfactual_prediction\/';
    var disqus_title = 'Establishing Causality with Counterfactual Prediction';
    var disqus_url = '\/portfolio\/counterfactual_prediction\/';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      </div>
    </div>
  </div>
</div>

          </div>
      </div>
  </div>
  <script src="/js/jquery.min.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/jquery.cookie.js"> </script>
<script src="/js/ekko-lightbox.js"></script>
<script src="/js/jquery.scrollTo.min.js"></script>
<script src="/js/masonry.pkgd.min.js"></script>
<script src="/js/imagesloaded.pkgd.min.js"></script>
<script src="/js/owl.carousel.min.js"></script>
<script src="/js/front.js"></script>

</body>
</html>
